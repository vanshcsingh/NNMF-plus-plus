{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCMF(nn.Module):\n",
    "    '''\n",
    "    Base class for Fully-Connected Matrix Factorization networks\n",
    "    '''\n",
    "    \n",
    "    def __init__ (N, M, D, D_, K, layers):\n",
    "        '''\n",
    "        variable definitions taken from paper: https://arxiv.org/pdf/1511.06443.pdf\n",
    "        \n",
    "        @param N:  Number of users\n",
    "        @param M:  Number of items\n",
    "        @param D:  size of latent-feature vectors\n",
    "        @param D_: num rows in latent-features matrices\n",
    "        @param K:  num cols in latent-feature matrices\n",
    "        \n",
    "        @param layers: list of hidden layer sizes; does not include input or output\n",
    "        '''\n",
    "        self.N, self.M, self.D, self.D_, self.K = N, M, D, D_, K\n",
    "        \n",
    "        self.userLatentVectors = torch.rand(N,D, requires_grad=True)\n",
    "        self.itemLatentVectors = torch.rand(M,D, requires_grad=True)\n",
    "        \n",
    "        self.userLatentMatrices = torch.rand(N,D_,K, requires_grad=True)\n",
    "        self.itemLatentMatrices = torch.rand(M,D_,K, requires_grad=True)\n",
    "        \n",
    "        linear_inputs = [2*D + D_] + layers\n",
    "        linear_outputs = layers + [1]\n",
    "        \n",
    "        self.layers = nn.ModuleList([nn.Linear(i,o) for (i,o) in zip(linear_inputs, linear_outputs)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @param x: let this be a tensor of size (1, 2): (user index, item index)\n",
    "        \n",
    "        WARNING: \n",
    "            - forward currently does not account for user/items outside of training data\n",
    "            - mitigations include returning smart averages    \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        userIndex, itemIndex = x[0][0].item(), x[0][1].item()\n",
    "        \n",
    "        latentDotProducts = torch.empty(self.D_)\n",
    "        for i in range(D_):\n",
    "            latentDotProducts[i] = self.userLatentMatrices[userIndex][i].dot(self.itemLatentMatrices[itemIndex[i]])\n",
    "        \n",
    "        x = torch.stack([\n",
    "            self.userLatentVectors[userIndex],\n",
    "            self.itemLatentVectors[itemIndex],\n",
    "            latentDotProducts\n",
    "        ])\n",
    "        \n",
    "        for l in self.layers[:-1]:\n",
    "            x = F.relu(l(x))\n",
    "        \n",
    "        # TODO: should last layer go through a sigmoid?\n",
    "        return self.layers[-1](x)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
