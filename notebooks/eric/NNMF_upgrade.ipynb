{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parameter as P\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "use_gpu = False\n",
    "if torch.cuda.is_available() and use_gpu:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data):\n",
    "        'Initialization'\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        data_item = self.data[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = data_item[:2]\n",
    "        X = torch.tensor(X).to(device)\n",
    "        y = data_item[2]\n",
    "        y = torch.tensor(y).to(device).float()\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIds = set()\n",
    "movieIds = set()\n",
    "triplets = []\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "         'drop_last': True}\n",
    "\n",
    "file1 = open('../../ml-100k/u.data', 'r')\n",
    "for line in file1.readlines():\n",
    "    uid, mid, rating, timestamp = line.split('\t')\n",
    "    userIds.add(int(uid))\n",
    "    movieIds.add(int(mid))\n",
    "    triplets.append([uid,mid,rating])\n",
    "    \n",
    "random.shuffle(triplets)\n",
    "\n",
    "triplets = np.array(triplets, dtype='int')\n",
    "\n",
    "train_val_split = int(len(triplets)*0.95)\n",
    "train_triplets = triplets[:train_val_split]\n",
    "val_triplets = triplets[train_val_split:]\n",
    "\n",
    "test_split = int(len(val_triplets)/2)\n",
    "\n",
    "test_triplets = val_triplets[:test_split]\n",
    "val_triplets = val_triplets[test_split:]\n",
    "\n",
    "\n",
    "training_set = Dataset(train_triplets)\n",
    "val_set = Dataset(val_triplets)\n",
    "test_set = Dataset(test_triplets)\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(training_set, **params)\n",
    "val_gen = torch.utils.data.DataLoader(val_set, **params)\n",
    "test_gen = torch.utils.data.DataLoader(test_set, **params)\n",
    "\n",
    "numUsers = max(userIds)\n",
    "numItems = max(movieIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        weight_range = 4.0 * pow(6, 0.5) / pow(m.in_features + m.out_features, 0.5)\n",
    "        # apply a paper distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(-1 * weight_range, weight_range)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCMF(nn.Module):\n",
    "    '''\n",
    "    Base class for Fully-Connected Matrix Factorization networks\n",
    "    '''\n",
    "\n",
    "    def __init__ (self, N, M, D, D_, K, layers):\n",
    "        '''\n",
    "        variable definitions taken from paper: https://arxiv.org/pdf/1511.06443.pdf\n",
    "        \n",
    "        @param N:  Number of users\n",
    "        @param M:  Number of items\n",
    "        @param D:  size of latent-feature vectors\n",
    "        @param D_: num rows in latent-features matrices\n",
    "        @param K:  num cols in latent-feature matrices\n",
    "        \n",
    "        @param layers: list of hidden layer sizes; does not include input or output\n",
    "        '''\n",
    "        \n",
    "        assert (min(N,M,D,D_,K) > 0), \"Params must be nonzero and positive\"\n",
    "        assert (len(layers) > 0),     \"Must have nonzero hidden layers\"\n",
    "        \n",
    "        ########################################################################\n",
    "        \n",
    "        super(FCMF, self).__init__()\n",
    "\n",
    "        \n",
    "        self.N, self.M, self.D, self.D_, self.K = N, M, D, D_, K\n",
    "\n",
    "        \n",
    "        self.userLatentVectors = P.Parameter(torch.rand(N,D, requires_grad=True))\n",
    "        self.itemLatentVectors = P.Parameter(torch.rand(M,D, requires_grad=True))\n",
    "\n",
    "        \n",
    "        self.userLatentMatrices = P.Parameter(torch.rand(N,D_,K, requires_grad=True))\n",
    "        self.itemLatentMatrices = P.Parameter(torch.rand(M,D_,K, requires_grad=True))\n",
    "\n",
    "        \n",
    "        linear_inputs = [2*D + D_] + layers\n",
    "        linear_outputs = layers + [1]\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(i,o) for (i,o) in zip(linear_inputs, linear_outputs)])\n",
    "        #Initialize weights as specified in paper\n",
    "        self.apply(weights_init_uniform)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @param x: let this be a tensor of size (X, 2): (user index, item index)\n",
    "        \n",
    "        WARNING: \n",
    "            - forward currently does not account for user/items outside of training data\n",
    "            - mitigations include returning smart averages    \n",
    "        '''   \n",
    "        #Get 100 user and item indices\n",
    "        userIndices, itemIndices = x[:,0].long(), x[:,1].long()\n",
    "        \n",
    "        \n",
    "        #Select the 10 dimensional rows for each user and item        \n",
    "        userLatMats = self.userLatentMatrices[userIndices]\n",
    "        itemLatMats = self.itemLatentMatrices[itemIndices]\n",
    "\n",
    "        #Take the product of these and sum it to get the feature U'n,1 * V'm,1 + ... + U'n,D * V'm,D\n",
    "        latentDotProducts = torch.sum(userLatMats * itemLatMats, dim=-1)\n",
    "        \n",
    "        x = torch.cat([\n",
    "            #D\n",
    "            self.userLatentVectors[userIndices],\n",
    "            #D\n",
    "            self.itemLatentVectors[itemIndices],\n",
    "            #D'\n",
    "            latentDotProducts\n",
    "        ], dim=1)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        for l in self.layers[:-1]:\n",
    "            x = torch.sigmoid(l(x))\n",
    "        \n",
    "        # TODO: should last layer go through a sigmoid? NO!\n",
    "        return self.layers[-1](x)\n",
    "    \n",
    "    def gradAll(self):\n",
    "        self._setGrads(True, True, True, True, True)\n",
    "    \n",
    "    def gradNetwork(self):\n",
    "        self._setGrads(False, False, False, False, True)\n",
    "\n",
    "    def gradLatent(self):\n",
    "        self._setGrads(True, True, True, True, False)\n",
    "\n",
    "    def _setGrads(self, userVec, itemVec, userMat, itemMat, net):\n",
    "        self.userLatentVectors.requires_grad = userVec\n",
    "        self.itemLatentVectors.requires_grad = itemVec\n",
    "        \n",
    "        self.userLatentMatrices.requires_grad = userMat\n",
    "        self.itemLatentMatrices.requires_grad = itemMat\n",
    "        \n",
    "        self.layers.requires_grad = net\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatches(X, Y, usersPerBatch=100):\n",
    "    '''\n",
    "    batchSize = min(N - start, usersPerBatch) * M\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    start = 0\n",
    "    while start < N:\n",
    "\n",
    "        if start+usersPerBatch + 1 < N:\n",
    "            batch_x = torch.tensor(X[start:start+usersPerBatch]).to(device)\n",
    "            batch_y = torch.tensor(Y[start:start+usersPerBatch]).to(device).float()\n",
    "            start += usersPerBatch\n",
    "            yield (batch_x, batch_y)\n",
    "\n",
    "        else:\n",
    "            batch_x = torch.tensor(X[start:]).to(device)\n",
    "            batch_y = torch.tensor(Y[start:]).to(device).float()\n",
    "            start += usersPerBatch\n",
    "            yield (batch_x, batch_y)\n",
    "            \n",
    "def trainEpoch(opt, criterion, model):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    totalLoss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_gen:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred_y = model(batch_x)\n",
    "        #RMSE in order to compare to paper\n",
    "        loss = pow(criterion(batch_y, pred_y.flatten()), 0.5)\n",
    "        totalLoss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return totalLoss/len(train_gen)\n",
    "\n",
    "def evaluate(criterion, model):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for batch_x, batch_y in val_gen:\n",
    "        pred_y = model(batch_x)\n",
    "        loss += pow(criterion(batch_y, pred_y.flatten()), 0.5)\n",
    "        matching = (torch.round(pred_y.detach()).flatten() == batch_y.flatten()).type(torch.uint8).sum()\n",
    "        matching = matching\n",
    "        accuracy += matching\n",
    "    return loss/len(val_gen), accuracy.item()/len(val_set)\n",
    "\n",
    "\n",
    "def calculate_test_loss(criterion, model):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for batch_x, batch_y in test_gen:\n",
    "        pred_y = model(batch_x)\n",
    "        loss += pow(criterion(batch_y, pred_y.flatten()), 0.5)\n",
    "        matching = (torch.round(pred_y.detach()).flatten() == batch_y.flatten()).type(torch.uint8).sum()\n",
    "        matching = matching\n",
    "        accuracy += matching\n",
    "    return loss/len(test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 Train Loss: 1.9696478843688965 Val Loss: 0.9828413724899292 Test Loss: 0.9759077429771423\n",
      "Epoch 5 Train Loss: 1.6964131593704224 Val Loss: 0.9713844060897827 Test Loss: 0.9574007987976074\n",
      "Epoch 10 Train Loss: 1.4302937984466553 Val Loss: 1.0696138143539429 Test Loss: 1.0417087078094482\n",
      "Epoch 15 Train Loss: 1.171729564666748 Val Loss: 1.0844494104385376 Test Loss: 1.0602811574935913\n",
      "Epoch 20 Train Loss: 0.9651563167572021 Val Loss: 1.1333884000778198 Test Loss: 1.109114170074463\n",
      "K: 1 Best Test Loss: 0.9399153590202332\n",
      "Epoch 0 Train Loss: 1.9267122745513916 Val Loss: 0.9671722650527954 Test Loss: 0.9557816982269287\n",
      "Epoch 5 Train Loss: 1.3767738342285156 Val Loss: 1.038909673690796 Test Loss: 1.0429307222366333\n",
      "Epoch 10 Train Loss: 0.8900201320648193 Val Loss: 1.1045626401901245 Test Loss: 1.116472601890564\n",
      "Epoch 15 Train Loss: 0.5983258485794067 Val Loss: 1.1232383251190186 Test Loss: 1.1369060277938843\n",
      "Epoch 20 Train Loss: 0.462705135345459 Val Loss: 1.1294316053390503 Test Loss: 1.1469041109085083\n",
      "K: 5 Best Test Loss: 0.9557816982269287\n",
      "Epoch 0 Train Loss: 1.8966115713119507 Val Loss: 0.9722756743431091 Test Loss: 0.9675553441047668\n",
      "Epoch 5 Train Loss: 1.3041787147521973 Val Loss: 1.0340479612350464 Test Loss: 1.0271632671356201\n",
      "Epoch 10 Train Loss: 0.7927515506744385 Val Loss: 1.1074844598770142 Test Loss: 1.130068302154541\n",
      "Epoch 15 Train Loss: 0.5480062961578369 Val Loss: 1.0986449718475342 Test Loss: 1.1237415075302124\n",
      "Epoch 20 Train Loss: 0.4406617283821106 Val Loss: 1.0817713737487793 Test Loss: 1.0946190357208252\n",
      "K: 10 Best Test Loss: 0.9659146070480347\n",
      "Epoch 0 Train Loss: 1.8998265266418457 Val Loss: 0.9676074385643005 Test Loss: 0.9660130739212036\n",
      "Epoch 5 Train Loss: 1.3405754566192627 Val Loss: 1.0243422985076904 Test Loss: 1.029074788093567\n",
      "Epoch 10 Train Loss: 0.872491180896759 Val Loss: 1.1080882549285889 Test Loss: 1.0810847282409668\n",
      "Epoch 15 Train Loss: 0.626207709312439 Val Loss: 1.09452486038208 Test Loss: 1.0916227102279663\n",
      "Epoch 20 Train Loss: 0.5092960000038147 Val Loss: 1.0985991954803467 Test Loss: 1.0953419208526611\n",
      "K: 15 Best Test Loss: 0.9614304900169373\n",
      "Epoch 0 Train Loss: 1.9012622833251953 Val Loss: 0.9645338654518127 Test Loss: 0.9710801243782043\n",
      "Epoch 5 Train Loss: 1.3789548873901367 Val Loss: 1.0303963422775269 Test Loss: 1.0194555521011353\n",
      "Epoch 10 Train Loss: 0.9151961207389832 Val Loss: 1.0766170024871826 Test Loss: 1.068494200706482\n",
      "Epoch 15 Train Loss: 0.6556077003479004 Val Loss: 1.0909013748168945 Test Loss: 1.0650098323822021\n",
      "Epoch 20 Train Loss: 0.5273050665855408 Val Loss: 1.0941625833511353 Test Loss: 1.0609568357467651\n",
      "K: 20 Best Test Loss: 0.9508848190307617\n",
      "Epoch 0 Train Loss: 1.9066596031188965 Val Loss: 0.9710067510604858 Test Loss: 0.9733662605285645\n",
      "Epoch 5 Train Loss: 1.4336860179901123 Val Loss: 1.0048198699951172 Test Loss: 0.995136559009552\n",
      "Epoch 10 Train Loss: 1.001726508140564 Val Loss: 1.0462526082992554 Test Loss: 1.056465983390808\n",
      "Epoch 15 Train Loss: 0.7222945094108582 Val Loss: 1.0567771196365356 Test Loss: 1.0738073587417603\n",
      "Epoch 20 Train Loss: 0.5780746340751648 Val Loss: 1.0600484609603882 Test Loss: 1.0855677127838135\n",
      "K: 25 Best Test Loss: 0.9555752277374268\n",
      "Epoch 0 Train Loss: 1.9142301082611084 Val Loss: 0.9574021697044373 Test Loss: 0.958368182182312\n",
      "Epoch 5 Train Loss: 1.433908462524414 Val Loss: 1.0049598217010498 Test Loss: 1.0170449018478394\n",
      "Epoch 10 Train Loss: 1.007108449935913 Val Loss: 1.0721138715744019 Test Loss: 1.0698403120040894\n",
      "Epoch 15 Train Loss: 0.7351287603378296 Val Loss: 1.0739918947219849 Test Loss: 1.0708247423171997\n",
      "Epoch 20 Train Loss: 0.5960918664932251 Val Loss: 1.0606609582901 Test Loss: 1.0644867420196533\n",
      "K: 30 Best Test Loss: 0.958368182182312\n",
      "Epoch 0 Train Loss: 1.9120169878005981 Val Loss: 0.9682913422584534 Test Loss: 0.9576373100280762\n",
      "Epoch 5 Train Loss: 1.5595279932022095 Val Loss: 0.9819777011871338 Test Loss: 0.9810808897018433\n",
      "Epoch 10 Train Loss: 1.1886749267578125 Val Loss: 1.04824697971344 Test Loss: 1.0481750965118408\n",
      "Epoch 15 Train Loss: 0.8797274231910706 Val Loss: 1.0611841678619385 Test Loss: 1.0528340339660645\n",
      "Epoch 20 Train Loss: 0.6924489736557007 Val Loss: 1.0696996450424194 Test Loss: 1.0621999502182007\n",
      "K: 35 Best Test Loss: 0.9458835124969482\n",
      "Epoch 0 Train Loss: 1.9347310066223145 Val Loss: 0.9758811593055725 Test Loss: 0.9599827527999878\n",
      "Epoch 5 Train Loss: 1.5977720022201538 Val Loss: 0.9628825783729553 Test Loss: 0.9603308439254761\n",
      "Epoch 10 Train Loss: 1.2797890901565552 Val Loss: 1.0232605934143066 Test Loss: 1.0095525979995728\n",
      "Epoch 15 Train Loss: 0.9582210779190063 Val Loss: 1.0491225719451904 Test Loss: 1.0303889513015747\n",
      "Epoch 20 Train Loss: 0.7478442788124084 Val Loss: 1.0559910535812378 Test Loss: 1.0477012395858765\n",
      "K: 40 Best Test Loss: 0.9560624957084656\n",
      "Epoch 0 Train Loss: 1.9385395050048828 Val Loss: 0.977360188961029 Test Loss: 0.965302586555481\n",
      "Epoch 5 Train Loss: 1.6088690757751465 Val Loss: 0.9619422554969788 Test Loss: 0.9615010023117065\n",
      "Epoch 10 Train Loss: 1.311471700668335 Val Loss: 0.9946866035461426 Test Loss: 0.9929206967353821\n",
      "Epoch 15 Train Loss: 0.9935996532440186 Val Loss: 1.0501251220703125 Test Loss: 1.0444056987762451\n",
      "Epoch 20 Train Loss: 0.7677915692329407 Val Loss: 1.0443812608718872 Test Loss: 1.049208164215088\n",
      "K: 45 Best Test Loss: 0.9537745714187622\n",
      "Epoch 0 Train Loss: 1.932327389717102 Val Loss: 0.9703510999679565 Test Loss: 0.9623112082481384\n",
      "Epoch 5 Train Loss: 1.6454834938049316 Val Loss: 0.9692541360855103 Test Loss: 0.9633497595787048\n",
      "Epoch 10 Train Loss: 1.3797802925109863 Val Loss: 1.0263020992279053 Test Loss: 1.0083390474319458\n",
      "Epoch 15 Train Loss: 1.055613398551941 Val Loss: 1.0665514469146729 Test Loss: 1.0358338356018066\n",
      "Epoch 20 Train Loss: 0.8109406232833862 Val Loss: 1.0751097202301025 Test Loss: 1.0663108825683594\n",
      "K: 50 Best Test Loss: 0.952860414981842\n"
     ]
    }
   ],
   "source": [
    "# Paper uses RMSE as objective and RMSProp optimizer\n",
    "\n",
    "\n",
    "test_losses = []\n",
    "Ks = []\n",
    "K = 1\n",
    "while K <= 50:\n",
    "    fc3 = FCMF(numUsers+1, numItems+1 ,10,60,K,[50, 50, 50]).to(device)\n",
    "    criterion = nn.MSELoss() \n",
    "    optimizer = optim.RMSprop(fc3.parameters(), lr=0.001)\n",
    "    epochs = 0\n",
    "    max_epochs = 25\n",
    "    best_test_loss = 10000\n",
    "    while epochs < max_epochs:\n",
    "        fc3.gradAll()\n",
    "        fc3.gradLatent()\n",
    "        loss = trainEpoch(optimizer, criterion, fc3)\n",
    "        fc3.gradNetwork()\n",
    "        loss += trainEpoch(optimizer, criterion, fc3)\n",
    "        val_loss, val_acc = evaluate(criterion, fc3)\n",
    "        test_loss = calculate_test_loss(criterion, fc3)\n",
    "        if epochs % 5 == 0:\n",
    "            print(\"Epoch {} Train Loss: {} Val Loss: {} Test Loss: {}\".format(epochs,loss, val_loss, test_loss))\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "        epochs+=1\n",
    "    test_losses.append(best_test_loss)\n",
    "    Ks.append(K)\n",
    "    print(\"K: {} Best Test Loss: {}\".format(K, best_test_loss))\n",
    "    if K == 1:\n",
    "        K = 5\n",
    "    else:\n",
    "        K += 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"K_sweep_5-50\", \"w\")\n",
    "for l,k in zip(test_losses, Ks):\n",
    "    f.write(\"{},{}\\n\".format(k,l))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7a1ad7128add87d22af7c72f064edfd2d9352ab762bb2d92657c175d8d0ed414"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}