{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parameter as P\n",
    "import torch.optim as optim\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data):\n",
    "        'Initialization'\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        data_item = self.data[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = data_item[:2]\n",
    "        X = torch.tensor(X).to(device)\n",
    "        y = data_item[2]\n",
    "        y = torch.tensor(y).to(device).float()\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIds = set()\n",
    "movieIds = set()\n",
    "triplets = []\n",
    "\n",
    "\n",
    "\n",
    "file1 = open('../../ml-100k/u.data', 'r')\n",
    "for line in file1.readlines():\n",
    "    uid, mid, rating, timestamp = line.split('\t')\n",
    "    userIds.add(int(uid))\n",
    "    movieIds.add(int(mid))\n",
    "    triplets.append([uid,mid,rating])\n",
    "\n",
    "    \n",
    "random.shuffle(triplets)\n",
    "\n",
    "triplets = np.array(triplets, dtype='int')\n",
    "\n",
    "train_val_split = int(len(triplets)*0.8)\n",
    "train_triplets = triplets[: train_val_split]\n",
    "val_triplets = triplets[train_val_split:]\n",
    "\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "         'drop_last': True}\n",
    "\n",
    "\n",
    "\n",
    "training_set = Dataset(train_triplets)\n",
    "val_set = Dataset(val_triplets)\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(training_set, **params)\n",
    "val_gen = torch.utils.data.DataLoader(val_set, **params)\n",
    "\n",
    "numUsers = max(userIds)\n",
    "numItems = max(movieIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        weight_range = 4.0 * pow(6, 0.5) / pow(m.in_features + m.out_features, 0.5)\n",
    "        # apply a paper distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(-1 * weight_range, weight_range)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCMF(nn.Module):\n",
    "    '''\n",
    "    Base class for Fully-Connected Matrix Factorization networks\n",
    "    '''\n",
    "\n",
    "    def __init__ (self, N, M, D, D_, K, layers):\n",
    "        '''\n",
    "        variable definitions taken from paper: https://arxiv.org/pdf/1511.06443.pdf\n",
    "        \n",
    "        @param N:  Number of users\n",
    "        @param M:  Number of items\n",
    "        @param D:  size of latent-feature vectors\n",
    "        @param D_: num rows in latent-features matrices\n",
    "        @param K:  num cols in latent-feature matrices\n",
    "        \n",
    "        @param layers: list of hidden layer sizes; does not include input or output\n",
    "        '''\n",
    "        \n",
    "        assert (min(N,M,D,D_,K) > 0), \"Params must be nonzero and positive\"\n",
    "        assert (len(layers) > 0),     \"Must have nonzero hidden layers\"\n",
    "        \n",
    "        ########################################################################\n",
    "        \n",
    "        super(FCMF, self).__init__()\n",
    "\n",
    "        \n",
    "        self.N, self.M, self.D, self.D_, self.K = N, M, D, D_, K\n",
    "\n",
    "        \n",
    "        self.userLatentVectors = P.Parameter(torch.rand(N,D, requires_grad=True))\n",
    "        self.itemLatentVectors = P.Parameter(torch.rand(M,D, requires_grad=True))\n",
    "\n",
    "        \n",
    "        self.userLatentMatrices = P.Parameter(torch.rand(N,D_,K, requires_grad=True))\n",
    "        self.itemLatentMatrices = P.Parameter(torch.rand(M,D_,K, requires_grad=True))\n",
    "\n",
    "        \n",
    "        linear_inputs = [2*D + D_] + layers\n",
    "        linear_outputs = layers + [1]\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(i,o) for (i,o) in zip(linear_inputs, linear_outputs)])\n",
    "        #Initialize weights as specified in paper\n",
    "        self.apply(weights_init_uniform)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        @param x: let this be a tensor of size (X, 2): (user index, item index)\n",
    "        \n",
    "        WARNING: \n",
    "            - forward currently does not account for user/items outside of training data\n",
    "            - mitigations include returning smart averages    \n",
    "        '''   \n",
    "        #Get 100 user and item indices\n",
    "        userIndices, itemIndices = x[:,0].long(), x[:,1].long()\n",
    "        \n",
    "        \n",
    "        #Select the 10 dimensional rows for each user and item        \n",
    "        userLatMats = self.userLatentMatrices[userIndices]\n",
    "        itemLatMats = self.itemLatentMatrices[itemIndices]\n",
    "\n",
    "        #Take the product of these and sum it to get the feature U'n,1 * V'm,1 + ... + U'n,D * V'm,D\n",
    "        latentDotProducts = torch.sum(userLatMats * itemLatMats, dim=-1)\n",
    "        \n",
    "        x = torch.cat([\n",
    "            #D\n",
    "            self.userLatentVectors[userIndices],\n",
    "            #D\n",
    "            self.itemLatentVectors[itemIndices],\n",
    "            #D'\n",
    "            latentDotProducts\n",
    "        ], dim=1)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        for l in self.layers[:-1]:\n",
    "            x = torch.sigmoid(l(x))\n",
    "        \n",
    "        # TODO: should last layer go through a sigmoid? NO!\n",
    "        out = self.layers[-1](x)\n",
    "        return out\n",
    "    \n",
    "    def gradAll(self):\n",
    "        self._setGrads(True, True, True, True, True)\n",
    "    \n",
    "    def gradNetwork(self):\n",
    "        self._setGrads(False, False, False, False, True)\n",
    "\n",
    "    def gradLatent(self):\n",
    "        self._setGrads(True, True, True, True, False)\n",
    "\n",
    "    def _setGrads(self, userVec, itemVec, userMat, itemMat, net):\n",
    "        self.userLatentVectors.requires_grad = userVec\n",
    "        self.itemLatentVectors.requires_grad = itemVec\n",
    "        \n",
    "        self.userLatentMatrices.requires_grad = userMat\n",
    "        self.itemLatentMatrices.requires_grad = itemMat\n",
    "        \n",
    "        self.layers.requires_grad = net\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatches(X, Y, usersPerBatch=100):\n",
    "    '''\n",
    "    batchSize = min(N - start, usersPerBatch) * M\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    start = 0\n",
    "    while start < N:\n",
    "\n",
    "        if start+usersPerBatch + 1 < N:\n",
    "            batch_x = torch.tensor(X[start:start+usersPerBatch]).to(device)\n",
    "            batch_y = torch.tensor(Y[start:start+usersPerBatch]).to(device).float()\n",
    "            start += usersPerBatch\n",
    "            yield (batch_x, batch_y)\n",
    "\n",
    "        else:\n",
    "            batch_x = torch.tensor(X[start:]).to(device)\n",
    "            batch_y = torch.tensor(Y[start:]).to(device).float()\n",
    "            start += usersPerBatch\n",
    "            yield (batch_x, batch_y)\n",
    "            \n",
    "def trainEpoch(opt, criterion, model):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    totalLoss = 0\n",
    "\n",
    "    for batch_x, batch_y in train_gen:\n",
    "        optimizer.zero_grad()\n",
    "        pred_y = model(batch_x)\n",
    "        #RMSE in order to compare to paper\n",
    "        loss = pow(criterion(batch_y, pred_y.flatten()), 0.5)\n",
    "        #reg_loss = torch.norm(model.userLatentVectors) + torch.norm(model.itemLatentVectors) + torch.norm(model.userLatentMatrices) + torch.norm(model.itemLatentMatrices)\n",
    "        #loss = pred_loss + 50 * reg_loss\n",
    "        totalLoss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return totalLoss / len(train_gen)\n",
    "\n",
    "def evaluate(criterion, model):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for batch_x, batch_y in val_gen:\n",
    "        pred_y = model(batch_x)\n",
    "        loss += pow(criterion(batch_y, pred_y.flatten()), 0.5)\n",
    "        matching = (torch.round(pred_y.detach()).flatten() == batch_y.flatten()).type(torch.uint8).sum()\n",
    "        matching = matching\n",
    "        accuracy += matching\n",
    "    return loss / len(val_gen), accuracy.item()/len(val_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FCMF(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=80, out_features=30, bias=True)\n",
      "    (1): Linear(in_features=30, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (3): Linear(in_features=30, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0 Train Loss: 1.9717001914978027 Val Loss: 0.9679232239723206 Val Acc: 0.39125 Duration: 29.685832738876343\n",
      "New min val loss: 0.9679232239723206. Saving model weights\n",
      "Epoch 1 Train Loss: 1.8457210063934326 Val Loss: 0.9542277455329895 Val Acc: 0.4096 Duration: 29.965392351150513\n",
      "New min val loss: 0.9542277455329895. Saving model weights\n",
      "Epoch 2 Train Loss: 1.8009288311004639 Val Loss: 0.9493799805641174 Val Acc: 0.40555 Duration: 29.98677635192871\n",
      "New min val loss: 0.9493799805641174. Saving model weights\n",
      "Epoch 3 Train Loss: 1.7624928951263428 Val Loss: 0.9509684443473816 Val Acc: 0.4 Duration: 30.076553344726562\n",
      "Epoch 4 Train Loss: 1.7210348844528198 Val Loss: 0.9577335715293884 Val Acc: 0.4119 Duration: 29.89827585220337\n",
      "Epoch 5 Train Loss: 1.675891637802124 Val Loss: 0.9999796748161316 Val Acc: 0.3771 Duration: 29.71635890007019\n",
      "Epoch 6 Train Loss: 1.6265454292297363 Val Loss: 0.9685107469558716 Val Acc: 0.40665 Duration: 29.914119243621826\n",
      "Epoch 7 Train Loss: 1.5764460563659668 Val Loss: 0.9898577928543091 Val Acc: 0.39895 Duration: 30.522226572036743\n",
      "Epoch 8 Train Loss: 1.5229291915893555 Val Loss: 1.0116736888885498 Val Acc: 0.40835 Duration: 29.921968698501587\n",
      "Epoch 9 Train Loss: 1.4701433181762695 Val Loss: 1.0238921642303467 Val Acc: 0.40335 Duration: 29.722843408584595\n",
      "Epoch 10 Train Loss: 1.4178657531738281 Val Loss: 1.0268268585205078 Val Acc: 0.39335 Duration: 29.524320363998413\n",
      "Epoch 11 Train Loss: 1.36638343334198 Val Loss: 1.044094443321228 Val Acc: 0.37865 Duration: 29.785568475723267\n",
      "Epoch 12 Train Loss: 1.315213680267334 Val Loss: 1.0620049238204956 Val Acc: 0.3885 Duration: 29.75087022781372\n",
      "Epoch 13 Train Loss: 1.26557457447052 Val Loss: 1.0665711164474487 Val Acc: 0.38535 Duration: 29.628628969192505\n",
      "Epoch 14 Train Loss: 1.2173902988433838 Val Loss: 1.0928587913513184 Val Acc: 0.36755 Duration: 29.65542984008789\n",
      "Epoch 15 Train Loss: 1.1720877885818481 Val Loss: 1.091281771659851 Val Acc: 0.37305 Duration: 29.45277190208435\n",
      "Epoch 16 Train Loss: 1.1262211799621582 Val Loss: 1.105684518814087 Val Acc: 0.37565 Duration: 29.37528157234192\n",
      "Epoch 17 Train Loss: 1.0847206115722656 Val Loss: 1.1148678064346313 Val Acc: 0.36885 Duration: 29.40660262107849\n",
      "Epoch 18 Train Loss: 1.0428123474121094 Val Loss: 1.1213436126708984 Val Acc: 0.3671 Duration: 29.287966012954712\n",
      "Epoch 19 Train Loss: 1.0033448934555054 Val Loss: 1.1408370733261108 Val Acc: 0.35665 Duration: 29.466626167297363\n",
      "Epoch 20 Train Loss: 0.9665945768356323 Val Loss: 1.1493135690689087 Val Acc: 0.36345 Duration: 29.403488874435425\n",
      "Epoch 21 Train Loss: 0.9320544004440308 Val Loss: 1.1546475887298584 Val Acc: 0.35715 Duration: 29.627281188964844\n",
      "Epoch 22 Train Loss: 0.8994326591491699 Val Loss: 1.1621192693710327 Val Acc: 0.3554 Duration: 29.69488024711609\n",
      "Epoch 23 Train Loss: 0.8671802282333374 Val Loss: 1.1687555313110352 Val Acc: 0.3519 Duration: 29.448631286621094\n",
      "Epoch 24 Train Loss: 0.8369737863540649 Val Loss: 1.193465232849121 Val Acc: 0.3433 Duration: 29.39449191093445\n",
      "Epoch 25 Train Loss: 0.809643566608429 Val Loss: 1.188931941986084 Val Acc: 0.3407 Duration: 29.471340894699097\n",
      "Epoch 26 Train Loss: 0.7832067012786865 Val Loss: 1.1978648900985718 Val Acc: 0.33795 Duration: 29.432589530944824\n",
      "Epoch 27 Train Loss: 0.7596148252487183 Val Loss: 1.2041057348251343 Val Acc: 0.34135 Duration: 29.42156457901001\n",
      "Epoch 28 Train Loss: 0.7366542220115662 Val Loss: 1.2253179550170898 Val Acc: 0.33375 Duration: 29.58396601676941\n",
      "Epoch 29 Train Loss: 0.7140485048294067 Val Loss: 1.2166383266448975 Val Acc: 0.33885 Duration: 29.307318449020386\n",
      "Epoch 30 Train Loss: 0.6923956274986267 Val Loss: 1.233458161354065 Val Acc: 0.33275 Duration: 29.869312524795532\n",
      "Epoch 31 Train Loss: 0.6741899251937866 Val Loss: 1.2279802560806274 Val Acc: 0.3373 Duration: 30.09888529777527\n",
      "Epoch 32 Train Loss: 0.6554790735244751 Val Loss: 1.229932188987732 Val Acc: 0.33565 Duration: 30.07202649116516\n",
      "Epoch 33 Train Loss: 0.6385518312454224 Val Loss: 1.2384686470031738 Val Acc: 0.3339 Duration: 30.621925592422485\n",
      "Epoch 34 Train Loss: 0.6220365762710571 Val Loss: 1.2481968402862549 Val Acc: 0.33185 Duration: 30.49803590774536\n",
      "Epoch 35 Train Loss: 0.6068670749664307 Val Loss: 1.239158034324646 Val Acc: 0.33895 Duration: 30.38916277885437\n",
      "Epoch 36 Train Loss: 0.5928225517272949 Val Loss: 1.2539242506027222 Val Acc: 0.33215 Duration: 29.845072507858276\n",
      "Epoch 37 Train Loss: 0.5782748460769653 Val Loss: 1.2713420391082764 Val Acc: 0.3291 Duration: 29.606072187423706\n",
      "Epoch 38 Train Loss: 0.565489649772644 Val Loss: 1.2520287036895752 Val Acc: 0.3356 Duration: 29.351484060287476\n",
      "Epoch 39 Train Loss: 0.551952064037323 Val Loss: 1.2508116960525513 Val Acc: 0.33785 Duration: 29.13616943359375\n",
      "Epoch 40 Train Loss: 0.5397487282752991 Val Loss: 1.2667090892791748 Val Acc: 0.33075 Duration: 29.14479947090149\n",
      "Epoch 41 Train Loss: 0.5291769504547119 Val Loss: 1.2634927034378052 Val Acc: 0.33265 Duration: 29.121432065963745\n",
      "Epoch 42 Train Loss: 0.5177901387214661 Val Loss: 1.2731492519378662 Val Acc: 0.32895 Duration: 29.287585496902466\n",
      "Epoch 43 Train Loss: 0.5078780055046082 Val Loss: 1.2855544090270996 Val Acc: 0.32395 Duration: 29.30698037147522\n",
      "Epoch 44 Train Loss: 0.4977095127105713 Val Loss: 1.2752420902252197 Val Acc: 0.3287 Duration: 29.43676996231079\n",
      "Epoch 45 Train Loss: 0.48911094665527344 Val Loss: 1.2649731636047363 Val Acc: 0.33365 Duration: 29.765977144241333\n",
      "Epoch 46 Train Loss: 0.4805906414985657 Val Loss: 1.2707659006118774 Val Acc: 0.33385 Duration: 30.03262686729431\n",
      "Epoch 47 Train Loss: 0.47142302989959717 Val Loss: 1.2723603248596191 Val Acc: 0.33195 Duration: 29.545174837112427\n",
      "Epoch 48 Train Loss: 0.4639299809932709 Val Loss: 1.2767314910888672 Val Acc: 0.33375 Duration: 29.635579586029053\n",
      "Epoch 49 Train Loss: 0.4563363790512085 Val Loss: 1.2815850973129272 Val Acc: 0.32755 Duration: 29.901591777801514\n"
     ]
    }
   ],
   "source": [
    "fc3 = FCMF(numUsers+1, numItems+1 ,10,60,1,[30, 50, 30]).to(device)\n",
    "# Paper uses RMSE as objective and RMSProp optimizer\n",
    "print(fc3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(fc3.parameters(), lr=0.001)\n",
    "min_val_loss = float('inf')\n",
    "path = \"NNMF/best_model.pt\"\n",
    "\n",
    "epochs = 0\n",
    "max_epochs = 50\n",
    "while epochs < max_epochs:\n",
    "    start = time.time()\n",
    "    fc3.gradAll()\n",
    "    fc3.gradLatent()\n",
    "    loss = trainEpoch(optimizer, criterion, fc3)\n",
    "    fc3.gradNetwork()\n",
    "    loss += trainEpoch(optimizer, criterion, fc3)\n",
    "    val_loss, val_acc = evaluate(criterion, fc3)\n",
    "    print(\"Epoch {} Train Loss: {} Val Loss: {} Val Acc: {} Duration: {}\".format(epochs,loss, val_loss, val_acc, time.time()-start))\n",
    "    epochs+=1\n",
    "    if val_loss < min_val_loss:\n",
    "        # torch.save(fc3.state_dict(), path)\n",
    "        print(f\"New min val loss: {val_loss}. Saving model weights\")\n",
    "        min_val_loss = val_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7a1ad7128add87d22af7c72f064edfd2d9352ab762bb2d92657c175d8d0ed414"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}